{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c20c7-1d8e-46a3-ac47-c85cc88d5140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import cv2\n",
    "import uuid \n",
    "import time\n",
    "import keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeb553f-c366-437f-b7b3-0356edf16db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'data/images'\n",
    "number_images = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88738851-0abf-4d7a-9eb6-b01a4c4d003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(0)\n",
    "# for i in range(number_images):\n",
    "#   ret, frame = cap.read()\n",
    "#   img_name = os.path.join(img_path, f'{str(uuid.uuid1())}.jpg')\n",
    "#   cv2.imwrite(img_name, frame)\n",
    "#   cv2.imshow('frame', frame)\n",
    "#   time.sleep(0.5)\n",
    "#   if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#     break\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bd97af-34bf-4157-99a7-d86e31e560db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ffbe34-b819-43e2-bf1d-9410d9bacd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(x):\n",
    "    img = tf.io.read_file(x)\n",
    "    img =tf.io.decode_jpeg(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cc5c78-1e8f-44cb-a5b1-3a81356aeb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = tf.data.Dataset.list_files('data/images/*.jpg', shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e45974-b7e7-4cdd-b766-efc5b8d6f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84481354-448a-4303-a6f2-8c2b38a33737",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = images.map(load_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8941966-17ee-4d22-8cca-c46b079e1e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img.as_numpy_iterator().next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc030f2-7ba0-499b-816c-3064a4c14036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "# images_path = glob.glob('data/images/*.jpg')\n",
    "# labels_path = glob.glob('data/labels/*.json')\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(images_path, labels_path, shuffle=False, train_size=0.85, random_state=0)\n",
    "# x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, shuffle=False, train_size=0.85, random_state=0)\n",
    "\n",
    "def split(x, y, c):\n",
    "    for source_path_img, source_path_label in zip(x, y):\n",
    "        shutil.copyfile(source_path_img, c + '/' + source_path_img.split('/')[1])\n",
    "        shutil.copyfile(source_path_label, c + '/' + source_path_label.split('/')[1])\n",
    "\n",
    "# split(x_train, y_train, 'data/train')\n",
    "# split(x_test, y_test, 'data/test')\n",
    "# split(x_val, y_val, 'data/val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93e6dbf-308f-475b-89b2-171472b734ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as alb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0a8883-4969-4127-aa17-aaae8139dff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imread('data/images/79400bd4-c452-11ed-abaf-7c214ae279f7.jpg').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76642a50-4f98-48c5-83a7-806321d72a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = alb.Compose([alb.RandomCrop(450, 450),\n",
    "                  alb.HorizontalFlip(0.5), \n",
    "                  alb.RandomBrightnessContrast(0.2), \n",
    "                  alb.RandomGamma(0.2), \n",
    "                  alb.RGBShift(p=0.5), \n",
    "                  alb.VerticalFlip(p=0.5)], \n",
    "                bbox_params=alb.BboxParams(format='albumentations', label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a94b22d-f486-4fac-a975-a9d8b268019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('data/images/7e4d6fc0-c452-11ed-ad16-7c214ae279f7.jpg')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73db5707-e4c1-4493-865f-ffceb29782af",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/labels/7e4d6fc0-c452-11ed-ad16-7c214ae279f7.json', 'r') as f:\n",
    "    label=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6f7c13-e8b6-4bae-b0b8-604591c62e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label['shapes'][0]['points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6077d5aa-f59e-465c-9f79-8bf0db55d34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coor = list(np.divide([311.025641025641, 177.43589743589743, 413.0769230769231, 301.53846153846155], [640, 480, 640, 480]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2dfa73-9b50-4d17-8b7e-dff5f9e888cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented = aug(image = img, bboxes=[coor], class_labels=['face'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebbbda2-a384-46ff-a866-a014626f8f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented['bboxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aff0c8e-5c2c-42c5-b734-cdcbb5b651d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.rectangle(augmented['image'], tuple(np.multiply(augmented['bboxes'][0][:2], [450, 450]).astype(int)), \n",
    "              tuple(np.multiply(augmented['bboxes'][0][2:], [450, 450]).astype(int)), (0, 0, 255), 4)\n",
    "plt.imshow(augmented['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e31ab4-14f3-4152-a52b-edd333093000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "for label, img in zip(glob.glob('data/labels/*.json'), glob.glob('data/images/*.jpg')):\n",
    "    imag = cv2.imread(img)\n",
    "    c = []\n",
    "    if os.path.exists(label):\n",
    "        with open(label, 'r') as f:\n",
    "            label_ = json.load(f)\n",
    "   \n",
    "        c.append(label_['shapes'][0]['points'][0][0])\n",
    "        c.append(label_['shapes'][0]['points'][0][1])\n",
    "        c.append(label_['shapes'][0]['points'][1][0])\n",
    "        c.append(label_['shapes'][0]['points'][1][1])\n",
    "        c = list(np.divide(c, [640,480,640,480]))   \n",
    "    \n",
    "    try:\n",
    "        for i in range(60):\n",
    "        \n",
    "            augmen = aug(image = imag, bboxes = [c], class_labels=['face'])\n",
    "            cv2.imwrite('aug_data/' + f'{img.split(\"/\")[1].split(\".\")[0]}_{i}.jpg', augmen['image'])\n",
    "        \n",
    "            j = {} \n",
    "            j['image'] = img.split('\\\\')[1]\n",
    "        \n",
    "            if os.path.exists(label):\n",
    "                if len(augmen['bboxes']) == 0:\n",
    "                    j['bbox'] = [0, 0, 0, 0]\n",
    "                    j['class'] = 0\n",
    "                else:\n",
    "                    j['bbox'] = augmen['bboxes'][0]\n",
    "                    j['class'] = 1\n",
    "            else: \n",
    "                j['bbox'] = [0, 0, 0, 0]\n",
    "                j['class'] = 0 \n",
    "   \n",
    "            with open ('aug_data/' + f'{label.split(\"/\")[1].split(\".\")[0]}_{i}.json', 'w') as f:\n",
    "                json.dump(j, f)\n",
    "    except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67238886-affb-4a1d-9b05-304cc9e10bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = glob.glob('aug_data/images/*.jpg')\n",
    "labels_path = glob.glob('aug_data/labels/*.json')\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(images_path, labels_path, shuffle=False, train_size=0.85, random_state=0)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, shuffle=False, train_size=0.85, random_state=0)\n",
    "\n",
    "split(x_train, y_train, \"aug_data/train\")\n",
    "split(x_test, y_test, 'aug_data/test')\n",
    "split(x_val, y_val, 'aug_data/val')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c477ee72-d53e-49ae-b58d-811aae1990f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbc30d0-d155-441a-b40f-2efa170d8821",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = tf.data.Dataset.list_files('aug_data\\\\train\\\\images\\\\*.jpg', shuffle=False)\n",
    "train_images = train_images.map(load_img)\n",
    "train_images = train_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "train_images = train_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1518fb46-c031-4975-964f-11ca3f3ff67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = tf.data.Dataset.list_files('aug_data\\\\test\\\\images\\\\*.jpg', shuffle=False)\n",
    "test_images = test_images.map(load_img)\n",
    "test_images = test_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "test_images = test_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136814a3-5d81-46e6-92b5-8cbd499ea818",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images = tf.data.Dataset.list_files('aug_data\\\\val\\\\images\\\\*.jpg', shuffle=False)\n",
    "val_images = val_images.map(load_img)\n",
    "val_images = val_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "val_images = val_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519cb798-c929-4187-94b6-3eabc924f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(label_path):\n",
    "    with open(label_path.numpy(), 'r', encoding = \"utf-8\") as f:\n",
    "        label = json.load(f)\n",
    "        \n",
    "    return [label['class']], label['bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5670c9be-1480-4f6f-a256-336c6595b89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = tf.data.Dataset.list_files('aug_data\\\\train\\\\labels\\\\*.json', shuffle=False)\n",
    "train_labels = train_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))\n",
    "test_labels = tf.data.Dataset.list_files('aug_data\\\\test\\\\labels\\\\*.json', shuffle=False)\n",
    "test_labels = test_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))\n",
    "val_labels = tf.data.Dataset.list_files('aug_data\\\\val\\\\labels\\\\*.json', shuffle=False)\n",
    "val_labels = val_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0e608a-4e86-4384-bff2-aac5f5f8ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_images), len(train_labels), len(test_images), len(test_labels), len(val_images), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f00c10d-8ac3-4160-a811-469241fb8819",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.data.Dataset.zip((train_images, train_labels))\n",
    "train = train.shuffle(5000)\n",
    "train = train.batch(8)\n",
    "train = train.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859b0262-1e11-4d25-a918-d77d87b6b00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = tf.data.Dataset.zip((val_images, val_labels))\n",
    "val = val.shuffle(1000)\n",
    "val = val.batch(8)\n",
    "val = val.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfa47ef-6fc5-43cf-bd09-7015031cb378",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tf.data.Dataset.zip((test_images, test_labels))\n",
    "test = test.shuffle(1300)\n",
    "test = test.batch(8)\n",
    "test = test.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c00b062-dbb2-42df-990c-67b5dc3713b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = train.as_numpy_iterator().next()\n",
    "cv2.rectangle(i[0][1],\n",
    "                  tuple(np.multiply(i[1][1][1][:2], [120,120]).astype(int)),\n",
    "                  tuple(np.multiply(i[1][1][1][2:], [120,120]).astype(int)), \n",
    "                        (255,0,0), 1)\n",
    "plt.imshow(i[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55697c2-6af3-4599-add2-501a51dbda77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea814993-20d1-489a-b80d-2fbe1d10d46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16(include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01530a2-8948-4fd0-96d4-c1e5a180d819",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfbae88-b2e0-4063-b88f-3de907824dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    input_layer = Input(shape=(120, 120, 3))\n",
    "    \n",
    "    vgg = VGG16(include_top=False)(input_layer)\n",
    "    \n",
    "    f1 = GlobalMaxPool2D()(vgg)\n",
    "    class1 = Dense(2048, activation='relu')(f1)\n",
    "    class2 = Dense(1, activation='sigmoid')(class1)\n",
    "    \n",
    "    f2 = GlobalMaxPool2D()(vgg)\n",
    "    regress1 = Dense(2048, activation='relu')(f2)\n",
    "    regress2 = Dense(4, activation='sigmoid')(regress1)    \n",
    "    \n",
    "    facetracker = Model(inputs = input_layer, outputs = [class2, regress2])\n",
    "    return facetracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9635a13-c304-4d71-8fe3-b0a9db40d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.as_numpy_iterator().next()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745c0ba6-2162-4e1d-8f47-3bf62c075fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker = build_model()\n",
    "facetracker.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a92f600-80d2-413a-b25f-80bd6a425edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7869027f-0059-4fae-ab8d-3cda469b78bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b874b3-1a0a-49e7-87ae-23f22d0b636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9050cec-67fd-4f27-aa26-77c0760ba302",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, coords = facetracker.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbe6536-c8b4-408b-a44d-5abe64c2f1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f8bdbc-ccd0-47cf-9055-4c7957bdf351",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch = len(train)\n",
    "lr_decay = (1/0.75-1)/batches_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd0e01b-5651-466c-9c07-3a76e6f9b6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=0.0001, decay=lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c9160a-0c93-4d04-b804-d3b747fa3e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def localization_loss(y_true, yhat):\n",
    "    delta_coord = tf.reduce_sum(tf.square(y_true[:, :2]-yhat[:, :2]))\n",
    "    \n",
    "    h_true = y_true[:, 3] - y_true[:, 1]\n",
    "    w_true = y_true[:, 2] - y_true[:, 0] \n",
    "    \n",
    "    h_pred = yhat[:, 3] - yhat[:, 1]\n",
    "    w_pred = yhat[:, 2] - yhat[:, 0]     \n",
    "\n",
    "    delta_size = tf.reduce_sum(tf.square(w_true - w_pred) + tf.square(h_true - h_pred))\n",
    "    \n",
    "    return delta_coord + delta_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712fed53-d9bb-45c0-9b77-5ebd93e23c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classloss = keras.losses.BinaryCrossentropy()\n",
    "regressloss = localization_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07345cc7-3d53-4999-8ae9-d7698ad9743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "localization_loss(y[1], coords).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0421b1ee-b5b1-496f-9761-8baf1ee46323",
   "metadata": {},
   "outputs": [],
   "source": [
    "classloss(y[0], classes).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50652558-814e-4796-9ab4-9e713f51619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressloss(y[1], coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52aecfa-9749-410d-8585-acd5b6e8f9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceTracker(Model): \n",
    "    def __init__(self, eyetracker,  **kwargs): \n",
    "        super().__init__(**kwargs)\n",
    "        self.model = eyetracker\n",
    "\n",
    "    def compile(self, opt, classloss, localizationloss, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.closs = classloss\n",
    "        self.lloss = localizationloss\n",
    "        self.opt = opt\n",
    "    \n",
    "    def train_step(self, batch, **kwargs): \n",
    "        \n",
    "        X, y = batch\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            classes, coords = self.model(X, training=True)\n",
    "            \n",
    "            batch_classloss = self.closs(y[0], classes)\n",
    "            batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
    "            \n",
    "            total_loss = batch_localizationloss+0.5*batch_classloss\n",
    "            \n",
    "            grad = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "        \n",
    "        opt.apply_gradients(zip(grad, self.model.trainable_variables))\n",
    "        \n",
    "        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regress_loss\":batch_localizationloss}\n",
    "    \n",
    "    def test_step(self, batch, **kwargs): \n",
    "        X, y = batch\n",
    "        \n",
    "        classes, coords = self.model(X, training=False)\n",
    "        \n",
    "        batch_classloss = self.closs(y[0], classes)\n",
    "        batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
    "        total_loss = batch_localizationloss+0.5*batch_classloss\n",
    "        \n",
    "        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regress_loss\":batch_localizationloss}\n",
    "        \n",
    "    def call(self, X, **kwargs): \n",
    "        return self.model(X, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9f210a-6c23-468d-99c9-66169acdba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FaceTracker(facetracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6fd2bb-dd25-40db-b4b8-a5da8e44c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(opt, classloss, regressloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48899a4e-3c90-48b9-bc27-d4587b73d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir='logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bc409e-1a78-419c-b669-86280bd9f3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d5ec7e-2e30-4423-b21e-f85855797edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train, epochs=5, validation_data=val, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a4411e-b8da-4e79-8316-0404bb6993a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3b49a1-0ede-479d-8e78-053b27da1226",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(20,5))\n",
    "\n",
    "ax[0].plot(hist.history['total_loss'], color='teal', label='loss')\n",
    "ax[0].plot(hist.history['val_total_loss'], color='orange', label='val loss')\n",
    "ax[0].title.set_text('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(hist.history['class_loss'], color='teal', label='class loss')\n",
    "ax[1].plot(hist.history['val_class_loss'], color='orange', label='val class loss')\n",
    "ax[1].title.set_text('Classification Loss')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].plot(hist.history['regress_loss'], color='teal', label='regress loss')\n",
    "ax[2].plot(hist.history['val_regress_loss'], color='orange', label='val regress loss')\n",
    "ax[2].title.set_text('Regression Loss')\n",
    "ax[2].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a956810-f7bc-4d02-8f9d-dca46f83d28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test.as_numpy_iterator()\n",
    "test_sample = test_data.next()\n",
    "yhat = facetracker.predict(test_sample[0])\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx in range(4): \n",
    "    sample_image = test_sample[0][idx]\n",
    "    sample_coords = yhat[1][idx]\n",
    "    \n",
    "    if yhat[0][idx] > 0.9:\n",
    "        cv2.rectangle(sample_image, \n",
    "                      tuple(np.multiply(sample_coords[:2], [120,120]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [120,120]).astype(int)), \n",
    "                            (255,0,0), 2)\n",
    "    \n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09f3ef2-3af5-4ee4-b6bc-e514a60d1f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fda1541-a358-4970-8ffd-07c936e9176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker.save('facetracker.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc36d71-dcde-4d9f-ba1f-fb4d9d215eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker = load_model('facetracker.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50f28bc-af5d-4c7c-804e-11c5e70d181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    _ , frame = cap.read()\n",
    "    frame = frame[50:500, 50:500,:]\n",
    "    \n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    resized = tf.image.resize(rgb, (120,120))\n",
    "    \n",
    "    yhat = facetracker.predict(np.expand_dims(resized/255,0))\n",
    "    sample_coords = yhat[1][0]\n",
    "    \n",
    "    if yhat[0] > 0.5: \n",
    "        # Controls the main rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.multiply(sample_coords[:2], [450,450]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [450,450]).astype(int)), \n",
    "                            (255,0,0), 2)\n",
    "        # Controls the label rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int), \n",
    "                                    [0,-30])),\n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                    [80,0])), \n",
    "                            (255,0,0), -1)\n",
    "        \n",
    "        # Controls the text rendered\n",
    "        cv2.putText(frame, 'face', tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                               [0,-5])),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow('EyeTrack', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af8d911-f238-4fff-8e63-263d1991d759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff40bad-d088-4190-a3f5-0371e92dda2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ai_gpu",
   "language": "python",
   "name": "ai_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
